{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an introduction to tensorflow, let's use tensorflow to classify MNIST numerals. However, we will not be using keras -- while keras simplifies constructing basic neural networks, it decreases flexibility that we may need in the future (e.g., when we try to model optical processes). I will try keep this introduction as simple as possible without including too many extra features that, while likely to be useful, may distract you from getting something running for the first time. Once we get the basics down, in the future we can revisit some of the helpful tools, such as graph visualization and tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load MNIST dataset:\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# verify that the shapes are correct:\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# cast as a float32; in general, you will work with float32 inputs:\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "# add a channels dimension:\n",
    "X_train = X_train[..., None]\n",
    "X_test = X_test[..., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# symbolically generate a batch of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be either X_train/y_train or X_test/y_test, so we make a placeholder that we can feed into:\n",
    "X_train_or_test = tf.placeholder(tf.float32, [None, 28, 28, 1], name='input_image')\n",
    "y_train_or_test = tf.placeholder(tf.int32, [None], name='image_label')\n",
    "batch_size = 32\n",
    "\n",
    "# create a tf dataset, from which we can generate batches\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train_or_test, y_train_or_test))\n",
    "dataset = dataset.batch(batch_size)\n",
    "batch_generator = dataset.make_initializable_iterator()\n",
    "X_batch, y_batch = batch_generator.get_next()  # batches symbolically generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# given a symbolic batch, symbolically process it through a network and output the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = X_batch\n",
    "\n",
    "# add some convolutional layers:\n",
    "net = tf.layers.conv2d(net, filters=32, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=32, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.max_pooling2d(net, pool_size=2, strides=2)\n",
    "\n",
    "# add some more if you want:\n",
    "net = tf.layers.conv2d(net, filters=64, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=64, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.max_pooling2d(net, pool_size=2, strides=2)\n",
    "\n",
    "# fully connected layers:\n",
    "net = tf.layers.flatten(net)\n",
    "net = tf.layers.dense(net, units=512, activation=tf.nn.relu)\n",
    "net = tf.layers.flatten(net)\n",
    "net = tf.layers.dense(net, units=10)\n",
    "\n",
    "logits = net\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(y_batch, depth=10), logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# given a loss, create an op that, when run, descends the gradient by one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start a tensorflow session and use it to initialize all <u>variables</u> and <u>ops</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do gradient descent: run a train loop over multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026054736\n",
      "0.0230975\n",
      "0.10829577\n",
      "0.09332211\n",
      "0.1412734\n"
     ]
    }
   ],
   "source": [
    "sess.run(batch_generator.initializer, feed_dict={X_train_or_test: X_train, y_train_or_test: y_train})\n",
    "for i in range(1000):\n",
    "    _, loss_i = sess.run([train_op, loss])\n",
    "    if i%100 == 0:\n",
    "        print(loss_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pass through the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9515625\n"
     ]
    }
   ],
   "source": [
    "sess.run(batch_generator.initializer, feed_dict={X_train_or_test: X_test, y_train_or_test: y_test})\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(100):\n",
    "    prediction, truth = sess.run([logits, y_batch])\n",
    "    correct += np.sum(prediction.argmax(1)==truth)\n",
    "    total += len(truth)\n",
    "acc = correct/total\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
